{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 1], [1, 5, 0], [0, 1, 1], [0, 3, 0], [0, 1, 1], [0, 2, 0], [1, 0, 1], [1, 1, 0], [1, 0, 1], [1, 5, 0], [0, 1, 1], [0, 5, 0], [0, 0, 1], [0, 3, 0], [0, 0, 1], [0, 5, 0], [0, 5, 1], [0, 3, 0], [5, 0, 1], [5, 1, 0], [5, 0, 1], [5, 0, 0], [0, 5, 1], [0, 5, 0], [0, 3, 1], [0, 4, 0], [3, 0, 1], [3, 0, 0], [3, 5, 1], [3, 2, 0], [5, 3, 1], [5, 3, 0], [5, 5, 1], [5, 3, 0], [5, 5, 1], [5, 5, 0], [5, 3, 1], [5, 2, 0], [3, 5, 1], [3, 1, 0], [3, 0, 1], [3, 2, 0], [0, 3, 1], [0, 0, 0], [0, 5, 1], [0, 5, 0], [5, 0, 1], [5, 4, 0], [5, 0, 1], [5, 3, 0], [0, 5, 1], [0, 4, 0], [0, 5, 1], [0, 5, 0], [5, 0, 1], [5, 0, 0], [5, 2, 1], [5, 0, 0], [2, 5, 1], [2, 0, 0], [2, 0, 1], [2, 1, 0], [0, 2, 1], [0, 2, 0], [0, 1, 1], [0, 5, 0], [1, 0, 1], [1, 2, 0], [1, 3, 1], [1, 4, 0], [3, 1, 1], [3, 5, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate):\n",
    "    \"\"\"\n",
    "    text - list of integer numbers - ids of tokens in text\n",
    "    window_size - odd integer - width of window\n",
    "    vocab_size - positive integer - number of tokens in vocabulary\n",
    "    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n",
    "\n",
    "    returns list of training samples (CenterWord, CtxWord, Label)\n",
    "    \"\"\"\n",
    "    def add_el(a, i, pos):\n",
    "        a.append([text[i], text[i + pos], 1])\n",
    "        for j in range(ns_rate):\n",
    "            a.append([text[i], np.random.randint(vocab_size), 0])\n",
    "        return a\n",
    "    \n",
    "    a = []\n",
    "    for i in range(len(text)):\n",
    "        \n",
    "        for j in range(-(window_size//2), 0):\n",
    "            if (i + j >=0) and (i + j < len(text)):\n",
    "                add_el(a,i,j)\n",
    "        for j in range(1,window_size//2+1):\n",
    "            if (i + j >=0) and (i + j < len(text)):\n",
    "                add_el(a,i,j)\n",
    "    return a\n",
    "\n",
    "\n",
    "text = [1, 0, 1, 0, 0, 5, 0, 3, 5, 5, 3, 0, 5, 0, 5, 2, 0, 1, 3]#read_array()\n",
    "window_size = 3#int(sys.stdin.readline().strip())\n",
    "vocab_size = 6#int(sys.stdin.readline().strip())\n",
    "ns_rate = 1#int(sys.stdin.readline().strip())\n",
    "\n",
    "result = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate)\n",
    "\n",
    "write_array(np.array(result))\n",
    "len(np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3449417709491044, 0.6762047256081501, 0.9583446027893963], [0.6247126159157468, 0.22038323197740317, 0.29717611444948355], [0.7561584406822226, 0.22438652516534294, -0.008774836618697823], [0.4217704869846559, 0.0023859008971685025, 0.009686915033163657], [0.6933070658521228, 0.9705089533296152, 0.9189360293193337], [0.024858486425111903, 0.11331113152689753, 0.6492144300167894], [0.7861289466352543, 0.227319130535791, 0.8165251907260063], [0.7672181161105678, 0.04865001026002924, 0.07514404284170773]]\n",
      "[[0.4628817426583818, 0.7747296319956671, 0.1374808935513827], [0.17026823169513283, 0.4094733988461122, 0.3175531656197459], [0.2910876746161247, 0.6340566555548147, 0.23158010794029804], [0.8449042648180852, 0.4796593509107806, 0.11278090182290745], [0.049097778744511156, 0.6254116250148337, 0.13038703647472905], [0.6290474670186392, 0.5231442006778062, 0.15471882755224034], [0.6704032810194875, 0.941803340812521, 0.7358646489592193], [0.9875878745059805, 0.17935677165390562, 0.6798846454394736]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def update_w2v_weights(center_embeddings, context_embeddings, center_word, context_word, label, learning_rate):\n",
    "    \"\"\"\n",
    "    center_embeddings - VocabSize x EmbSize\n",
    "    context_embeddings - VocabSize x EmbSize\n",
    "    center_word - int - identifier of center word\n",
    "    context_word - int - identifier of context word\n",
    "    label - 1 if context_word is real, 0 if it is negative\n",
    "    learning_rate - float > 0 - size of gradient step\n",
    "    \"\"\"\n",
    "    #your code here - update center_embeddings and context_embeddings inplace\n",
    "    import math\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    s = sigmoid((center_embeddings[center_word]*context_embeddings[context_word]).sum())\n",
    "    \n",
    "    if label == 0:\n",
    "        ce = center_embeddings[center_word] - learning_rate*s*context_embeddings[context_word]\n",
    "        context_embeddings[context_word] = context_embeddings[context_word] - learning_rate*s*center_embeddings[center_word]\n",
    "        center_embeddings[center_word] = ce\n",
    "    else:\n",
    "        ce = center_embeddings[center_word] + learning_rate*(1-s)*context_embeddings[context_word]\n",
    "        context_embeddings[context_word] = context_embeddings[context_word] + learning_rate*(1-s)*center_embeddings[center_word]\n",
    "        center_embeddings[center_word] = ce\n",
    "\n",
    "\n",
    "center_embeddings = np.array([[0.3449417709491044, 0.6762047256081501, 0.9583446027893963], [0.6247126159157468, 0.22038323197740317, 0.29717611444948355], [0.9836099232994968, 0.3847689688960674, 0.033312247867206435], [0.4217704869846559, 0.0023859008971685025, 0.009686915033163657], [0.6933070658521228, 0.9705089533296152, 0.9189360293193337], [0.024858486425111903, 0.11331113152689753, 0.6492144300167894], [0.7861289466352543, 0.227319130535791, 0.8165251907260063], [0.7672181161105678, 0.04865001026002924, 0.07514404284170773]])\n",
    "context_embeddings = np.array([[0.4628817426583818, 0.7747296319956671, 0.1374808935513827], [0.17026823169513283, 0.4094733988461122, 0.3175531656197459], [0.2910876746161247, 0.6340566555548147, 0.23158010794029804], [0.8449042648180852, 0.4796593509107806, 0.11278090182290745], [0.049097778744511156, 0.6254116250148337, 0.13038703647472905], [0.882545488649187, 0.6223076699449618, 0.1633041302523962], [0.6704032810194875, 0.941803340812521, 0.7358646489592193], [0.9875878745059805, 0.17935677165390562, 0.6798846454394736]])\n",
    "center_word = 2\n",
    "context_word = 5\n",
    "label = 0\n",
    "learning_rate = 0.342405260598321\n",
    "\n",
    "update_w2v_weights(center_embeddings, context_embeddings,\n",
    "                   center_word, context_word, label, learning_rate)\n",
    "\n",
    "write_array(center_embeddings)\n",
    "write_array(context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 10, 12], 2, 1], [[1, 10, 12], 2, 0], [[1, 10, 12], 3, 0], [[2, 20], 1, 1], [[2, 20], 5, 0], [[2, 20], 0, 0], [[2, 20], 0, 1], [[2, 20], 3, 0], [[2, 20], 4, 0], [[0, 17], 2, 1], [[0, 17], 0, 0], [[0, 17], 4, 0], [[0, 17], 1, 1], [[0, 17], 1, 0], [[0, 17], 4, 0], [[1, 10, 12], 0, 1], [[1, 10, 12], 0, 0], [[1, 10, 12], 3, 0], [[1, 10, 12], 4, 1], [[1, 10, 12], 0, 0], [[1, 10, 12], 0, 0], [[4], 1, 1], [[4], 1, 0], [[4], 3, 0], [[4], 0, 1], [[4], 1, 0], [[4], 3, 0], [[0, 17], 4, 1], [[0, 17], 0, 0], [[0, 17], 5, 0], [[0, 17], 4, 1], [[0, 17], 5, 0], [[0, 17], 5, 0], [[4], 0, 1], [[4], 3, 0], [[4], 5, 0], [[4], 1, 1], [[4], 4, 0], [[4], 1, 0], [[1, 10, 12], 4, 1], [[1, 10, 12], 4, 0], [[1, 10, 12], 5, 0], [[1, 10, 12], 5, 1], [[1, 10, 12], 2, 0], [[1, 10, 12], 1, 0], [[5, 7, 11], 1, 1], [[5, 7, 11], 5, 0], [[5, 7, 11], 0, 0], [[5, 7, 11], 4, 1], [[5, 7, 11], 3, 0], [[5, 7, 11], 4, 0], [[4], 5, 1], [[4], 5, 0], [[4], 1, 0], [[4], 5, 1], [[4], 4, 0], [[4], 0, 0], [[5, 7, 11], 4, 1], [[5, 7, 11], 1, 0], [[5, 7, 11], 1, 0], [[5, 7, 11], 4, 1], [[5, 7, 11], 2, 0], [[5, 7, 11], 1, 0], [[4], 5, 1], [[4], 3, 0], [[4], 2, 0], [[4], 5, 1], [[4], 3, 0], [[4], 2, 0], [[5, 7, 11], 4, 1], [[5, 7, 11], 5, 0], [[5, 7, 11], 1, 0], [[5, 7, 11], 1, 1], [[5, 7, 11], 3, 0], [[5, 7, 11], 5, 0], [[1, 10, 12], 5, 1], [[1, 10, 12], 1, 0], [[1, 10, 12], 5, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_list():\n",
    "    return ast.literal_eval(sys.stdin.readline())\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def generate_ft_sgns_samples(text, window_size, vocab_size, ns_rate, token2subwords):\n",
    "    \"\"\"\n",
    "    text - list of integer numbers - ids of tokens in text\n",
    "    window_size - odd integer - width of window\n",
    "    vocab_size - positive integer - number of tokens in vocabulary\n",
    "    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n",
    "    token2subwords - list of lists of int - i-th sublist contains list of identifiers of n-grams for token #i (list of subword units)\n",
    "\n",
    "    returns list of training samples (CenterSubwords, CtxWord, Label)\n",
    "    \"\"\"\n",
    "    def add_el(a, i, pos):\n",
    "        centre = [text[i]]\n",
    "        for sw in token2subwords[text[i]]:\n",
    "            if not sw in centre:\n",
    "                centre.append(sw)\n",
    "            \n",
    "        a.append([centre, text[i + pos], 1])\n",
    "        for j in range(ns_rate):\n",
    "            a.append([centre, np.random.randint(vocab_size), 0])\n",
    "        return a\n",
    "    \n",
    "    a = []\n",
    "    for i in range(len(text)):\n",
    "        \n",
    "        for j in range(-(window_size//2), 0):\n",
    "            if (i + j >=0) and (i + j < len(text)):\n",
    "                add_el(a,i,j)\n",
    "        for j in range(1,window_size//2+1):\n",
    "            if (i + j >=0) and (i + j < len(text)):\n",
    "                add_el(a,i,j)\n",
    "    return a\n",
    "\n",
    "\n",
    "text = [1, 2, 0, 1, 4, 0, 4, 1, 5, 4, 5, 4, 5, 1]\n",
    "window_size = 3\n",
    "vocab_size = 6\n",
    "ns_rate = 2\n",
    "token2subwords = [[17], [10, 12], [20, 20], [7, 13], [], [7, 11]]\n",
    "\n",
    "result = generate_ft_sgns_samples(text, window_size, vocab_size, ns_rate, token2subwords)\n",
    "\n",
    "print(repr(result))\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
